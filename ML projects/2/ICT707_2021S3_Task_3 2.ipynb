{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICT707 Task 3 - Part 1 \n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Follow instructions on 'Google Colab for Task3.doc' for running this notebook:\n",
    "\n",
    "\n",
    "* 1.\tVisit and log in to Google Colab site: https://colab.research.google.com/\n",
    "* 2.\tDownload this notebook from Blackboard and then upload it to Colab\n",
    "* 3. \tRun “PySpark Environment Setting” cell to get spark and pySpark installed.\n",
    "* 4.\tType in your NAME and ID in the first coding cell.\n",
    "* 5.\tPlace the data files on the correct GDrive folder - 'Colab Notebooks'.\n",
    "* 6. \tRun the first cell of “Connect GDrive for data set files” to mount GDrive as the storage of data files. Follow the instruction to complete the authorization of using GDrive. \n",
    "* 7.    Run Imports\n",
    "* 8.    Create Spark Session\n",
    "* 9.    Load CSV file and test\n",
    "\n",
    "* **After you finish, make sure all cells are executed. Go to menu \"File->Download .ipynb\" to download your work as 2 files: (1) a Jupyter notebook file and (2) a HTML file. And then submit both files to Blackboard.**\n",
    "\n",
    "* If you see any error related to spark context, please **run the last cell** and then retry. Or reload the notebook and install the PySpark environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Task 3 Setup\n",
    "### 1 PySpark Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoRLSnOtBXzn"
   },
   "outputs": [],
   "source": [
    "# Please run this cell to get Java and spark installed\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
    "!pip install pyspark==2.4.7\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsr_Xl5LBVNr"
   },
   "source": [
    "### 2 Enter your NAME and ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "czZCbsXABVNs"
   },
   "outputs": [],
   "source": [
    "# Please enter your NAME and student ID\n",
    "NAME = \"\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Add data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have relevant data files uploaded, replace 'text_file_name.csv' with your csv file\n",
    "# And then use the correct data file names below\n",
    "datafile = \"/content/gdrive/My Drive/Colab Notebooks/text_file_name.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF2ByTgJBnIE"
   },
   "source": [
    "### 4 Connect GDrive for data set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muPOGMTfBr2s"
   },
   "outputs": [],
   "source": [
    "# Mount the cloud folder for data file storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Run Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports utilised\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "sc = SparkSession.builder\\\n",
    "        .master('local[*]') \\\n",
    "        .appName('ICT707_Task3') \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Load CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxrL34e0BVNw"
   },
   "outputs": [],
   "source": [
    "# file should display the head() records without errors \n",
    "# Loading csv file for PySpark and Python 3\n",
    "data = sc.read.csv(datafile, inferSchema = True, header = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQqKWtAoBVN2"
   },
   "source": [
    "## 1 Exploratory Data Analysis\n",
    "##### • telling its number of rows and columns,\n",
    "##### • doing the data cleaning (missing values or duplicated records) if necessary\n",
    "##### • selecting 3 columns, and drawing 1 plot (e.g. bar chart, histogram, boxplot, etc.) for each to summarise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qre3g_psBVN3"
   },
   "outputs": [],
   "source": [
    "# To be able to visualise the data, the dataframe must be transferred to pandas\n",
    "data_pandas = data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 EDA - Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telling its number of rows and columns,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 EDA - Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the data cleaning (missing values or duplicated records) if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 EDA - Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting 3 columns, and drawing 1 plot (e.g. bar chart, histogram, boxplot, etc.) for each to summarise it\n",
    "# graph 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Recommendation Engine\n",
    "#### This subtask requires you to implement a recommender system on Collaborative filtering with Alternative Least Squares Algorithm. \n",
    "#### You need to include\n",
    "##### • Model training and predictions\n",
    "##### • Model evaluation using MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into Training and Test data that is used for both Task I.2 and Task I.3\n",
    "# hint: training_data, testing_data = data.randomSplit([???,???])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommendation system using Alternative Least Squares Algorithm\n",
    "# hint: als = ALS(maxIter=??,regParam=??,userCol = \"???\", itemCol = \"???\", ratingCol = \"???\", coldStartStrategy = \"???\")\n",
    "# hint: model = als.fit(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model using Mean Square Error\n",
    "# hint: predictions = model.transform(????)\n",
    "# hint: evaluator = RegressionEvaluator(?????)\n",
    "# hint: mse = evaluator.evaluate(????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Classification\n",
    "#### This subtask requires you to implement a classification system with Logistic regression. \n",
    "#### You need to include\n",
    "##### • Logistic Regression model training\n",
    "##### • Model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# hint: assembler = VectorAssembler().setInputCols([???????,????]).setOutputCol(????)\n",
    "# hint: train_vector = assembler.transform(?????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Logistic Regression Model and train it\n",
    "# hint: lr = LogisticRegression()\n",
    "# hint: lr_model = lr.fit(?????)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# hint: test_vector = assembler.transform(testing)\n",
    "# hint: test_vector = test_vector.select(\"features\", \"label\")\n",
    "# hint: test_vector = lr_model.transform(test_vector)\n",
    "# hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation using Root Mean Square Error (RMSE)\n",
    "# hint: rmse_test = evaluator.evaluate(?????, {evaluator.metricName: \"rmse\"})\n",
    "# hint: rmse_train = evaluator.evaluate(?????, {evaluator.metricName: \"rmse\"})\n",
    "#print(\"RMSE for Test:\",rmse_test)\n",
    "#print(\"RMSE for Train:\",rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inodh_mrBVN_"
   },
   "source": [
    "## Shut down SparkContext when exiting\n",
    "\n",
    "If you have error messages related to sparkContext, try to run the following cell, and then rerun all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IRm1qMaBVN_"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ICT707_2020S2_Task_3_Template_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
